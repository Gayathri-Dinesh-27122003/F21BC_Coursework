# PSO-ANN: Particle Swarm Optimization for Neural Network Training

Optimizing Artificial Neural Networks using Particle Swarm Optimization on the Concrete Compressive Strength Dataset.

## ðŸ“‹ Quick Overview

This project demonstrates PSO-ANN integration:
- **PSO** (Particle Swarm Optimization) optimizes network weights
- **ANN** (Artificial Neural Network) predicts concrete compressive strength
- **Dataset**: UCI ML Repository - Concrete Compressive Strength (1030 samples, 8 features)
- **Result**: ~15% RMSE improvement over random initialization

---

## ðŸš€ Quick Start (Choose One Option)

### Option 1: Run Locally (Requires Dependencies)
```bash
# Clone the repository
git clone https://github.com/Gayathri-Dinesh-27122003/F21BC_Coursework.git
cd F21BC_Coursework

# Install dependencies
pip install -r requirements.txt

# Run the demo
python3 src/main.py
```

### Option 2: Run Without Installing Dependencies (Recommended for Professor)
Using Docker (no local installation needed):

```bash
# Build Docker image
docker build -t pso-ann .

# Run the demo inside container
docker run --rm pso-ann python3 src/main.py
```

Or using Conda (clean environment):
```bash
# Create isolated environment
conda create -n pso-ann python=3.9

# Activate environment
conda activate pso-ann

# Install dependencies
pip install -r requirements.txt

# Run the demo
python3 src/main.py
```

Or using Python Virtual Environment:
```bash
# Create virtual environment
python3 -m venv venv

# Activate it
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the demo
python3 src/main.py
```

---

## ðŸ“ Project Structure & File Descriptions

### Core Source Files

#### `src/main.py` (152 lines) - **DEMO APPLICATION**
**Purpose**: Professional demonstration of PSO-ANN integration
**What it does**:
- Loads concrete dataset (824 training, 206 test samples)
- Creates a 3-layer neural network [8â†’64â†’32â†’1]
- Initializes PSO with 30 particles
- Runs 50 PSO iterations to optimize weights
- Displays results with performance metrics

**Run**:
```bash
cd src && python3 main.py
```

**Expected Output**:
```
Initial RMSE (random weights):    38.88
Final RMSE (optimized):            33.07
Improvement:                       14.93%
Train-Test Gap:                    -1.27 (good generalization)
```

**Runtime**: 2-3 minutes

---

#### `src/pso_ann_trainer.py` (62 lines) - **UTILITY MODULE**
**Purpose**: Provides reusable utility functions for PSO-ANN optimization
**Functions provided**:
- `to_numpy(arr)` - Convert array data types to numeric numpy arrays
- `rmse(y_true, y_pred)` - Calculate Root Mean Squared Error
- `create_fitness_function(network, X_train, y_train)` - Create PSO fitness wrapper

**Use in code**:
```python
from pso_ann_trainer import rmse, create_fitness_function, to_numpy

# Load data
data = np.load('processed_data.npz', allow_pickle=True)
X_train = to_numpy(data['X_train'])
X_test = to_numpy(data['X_test'])

# Create network and fitness function
network = NeuralNetwork([8, 64, 32, 1], ['relu', 'relu', 'linear'])
fitness = create_fitness_function(network, X_train, y_train)

# Calculate RMSE
error = rmse(y_test_true, y_test_pred)
```

**No Direct Run** (used by other modules)

---

#### `src/ann.py` (241 lines) - **NEURAL NETWORK**
**Purpose**: Multi-layer perceptron with configurable architecture
**Key Classes**:
- `NetworkArchitecture` - Defines layer structure and activation functions
- `ActivationFunctions` - ReLU and linear activations with derivatives
- `NeuralNetwork` - Complete forward pass and parameter management

**Key Methods**:
- `forward(X)` - Forward pass through all layers
- `predict(X)` - Forward pass with output flattening
- `set_parameters(vector)` - Load flat parameter vector into network
- `get_parameters()` - Extract flat parameter vector from network
- `get_parameter_count()` - Get total number of weights/biases

**Run Example**:
```python
from ann import NeuralNetwork
import numpy as np

# Create network
net = NeuralNetwork([8, 64, 32, 1], ['relu', 'relu', 'linear'])

# Create dummy input (8 features)
X = np.random.randn(10, 8)

# Forward pass
y_pred = net.predict(X)
print(f"Output shape: {y_pred.shape}")  # (10,)
```

**No Direct Run** (used by PSO and main.py)

---

#### `src/pso.py` (175 lines) - **PARTICLE SWARM OPTIMIZER**
**Purpose**: Informant-based PSO implementation (Algorithm 39)
**Key Classes**:
- `Particle` - Individual particle with position, velocity, personal best
- `PSO` - Main optimizer with swarm and particle updates

**Key Methods**:
- `optimize(verbose=False)` - Run PSO optimization loop
- `get_optimization_summary()` - Get convergence statistics

**Algorithm**:
- Each particle explores the weight space
- Particles move toward personal and social best positions
- Informant-based topology for local social learning
- Convergence within N iterations

**Run Example**:
```python
from pso import PSO
from ann import NeuralNetwork

# Create fitness function (minimize RMSE)
def fitness(params):
    network.set_parameters(params)
    pred = network.predict(X_train)
    return rmse(y_train, pred)

# Create and run PSO
pso = PSO(
    objective_function=fitness,
    dimension=2689,
    swarm_size=30,
    num_informants=3,
    max_iterations=50,
    bounds=(-5.0, 5.0)
)

best_params, best_fitness, history = pso.optimize(verbose=True)
```

**No Direct Run** (used by main.py)

---

### Jupyter Notebooks

#### `src/experiment_runner.ipynb` - **BATCH EXPERIMENTS**
**Purpose**: Run 10 different PSO-ANN experiments with visualizations
**What it contains**:
- Section 1: Imports
- Section 2: Load Data
- Section 3: Define 10 Experiment Configurations
- Section 4: Utility Functions (run_single_experiment)
- Sections 5-14: Individual experiment cells (one per experiment)
- Section 15: Visualizations (4-panel dashboard)
- Section 16: Statistical Analysis
- Section 17: Results Export (CSV + JSON)

**Experiment Configurations**:
1. Exp 1: [8,64,1] - Small network
2. Exp 2: [8,128,32,1] - Medium network
3. Exp 3: [8,64,64,1] - Wider network
4. ... (10 total with varying seeds, layer sizes, PSO parameters)
5. Exp 10: [8,200,100,1] - Large network

**Run**:
```bash
# Option 1: Open in VS Code
# Press Ctrl+Shift+P â†’ "Jupyter: Create New Blank Notebook"
# Then open experiment_runner.ipynb
# Click "Run All" or execute cells sequentially

# Option 2: Open in Jupyter Lab
jupyter lab src/experiment_runner.ipynb
# Then execute cells

# Option 3: Open in Jupyter Notebook
jupyter notebook src/experiment_runner.ipynb
# Then execute cells
```

**Output**:
- Results table (10 experiments with metrics)
- 4-panel visualization:
  - Panel 1: PSO convergence curves
  - Panel 2: Train vs Test RMSE comparison
  - Panel 3: Improvement percentage by experiment
  - Panel 4: RMSE distribution boxplot
- CSV export: `../results/experiment_results.csv`
- JSON export: `../results/experiment_results.json`
- PNG export: `../results/experiment_results.png`

**Runtime**: 15-20 minutes

---

#### `src/data_loader.ipynb` - **DATA PREPROCESSING**
**Purpose**: Load concrete dataset from UCI ML Repository and preprocess
**What it does**:
- Downloads Concrete Compressive Strength dataset
- Splits into train/test sets
- Standardizes features using StandardScaler
- Saves processed data to `data/processed_data.npz`

**Run**:
```bash
jupyter notebook src/data_loader.ipynb
# Execute cells sequentially
```

**Note**: Already executed. Data is available in `data/processed_data.npz`

**Runtime**: 2 minutes (download + preprocess)

---

#### `src/run_ann_example.py` (Simple ANN Test)
**Purpose**: Simple demonstration of forward pass without PSO
**What it does**:
- Creates a neural network with random weights
- Loads concrete dataset
- Performs forward pass
- Calculates RMSE

**Run**:
```bash
cd src && python3 run_ann_example.py
```

**Output**: RMSE with random weights

**Runtime**: 30 seconds

---

### Data Files

#### `data/processed_data.npz`
**Contents**:
- `X_train` - Training features (824 samples, 8 features) - standardized
- `X_test` - Test features (206 samples, 8 features) - standardized
- `y_train` - Training targets (824 values) - concrete strength in MPa
- `y_test` - Test targets (206 values) - concrete strength in MPa

**Source**: UCI ML Repository - Concrete Compressive Strength
**Dataset ID**: 165
**Features**: 8 (cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate, age)

#### `data/scaler.joblib`
**Contents**: StandardScaler object used to standardize features
**Use**: For scaling new data with same transformation

---

### Configuration Files

#### `requirements.txt`
**Dependencies**:
```
numpy>=1.20.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
joblib>=1.0.0
tqdm>=4.50.0
```

**Install**:
```bash
pip install -r requirements.txt
```

---

## ðŸŽ¯ How Professors Should Run This

### Option A: Using Docker (Easiest - No Dependencies)

**Step 1: Install Docker**
- Download from https://www.docker.com/products/docker-desktop
- Install and verify: `docker --version`

**Step 2: Clone Repository**
```bash
git clone https://github.com/Gayathri-Dinesh-27122003/F21BC_Coursework.git
cd F21BC_Coursework
```

**Step 3: Create Dockerfile** (if not present)
```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

WORKDIR /app/src
CMD ["python3", "main.py"]
```

**Step 4: Build and Run**
```bash
# Build image (one time)
docker build -t pso-ann .

# Run the demo
docker run --rm pso-ann
```

**Advantage**: No dependencies on host machine, clean isolated environment

---

### Option B: Using Conda (Recommended Alternative)

**Step 1: Install Conda**
- Download Anaconda/Miniconda from https://www.anaconda.com/download
- Install and verify: `conda --version`

**Step 2: Clone Repository**
```bash
git clone https://github.com/Gayathri-Dinesh-27122003/F21BC_Coursework.git
cd F21BC_Coursework
```

**Step 3: Create Environment**
```bash
conda create -n pso-ann python=3.9 --yes
conda activate pso-ann
pip install -r requirements.txt
```

**Step 4: Run**
```bash
python3 src/main.py
```

**Step 5: Deactivate When Done**
```bash
conda deactivate
```

**Advantage**: Easy environment management, can switch between projects

---

### Option C: Using Python Virtual Environment

**Step 1: Clone Repository**
```bash
git clone https://github.com/Gayathri-Dinesh-27122003/F21BC_Coursework.git
cd F21BC_Coursework
```

**Step 2: Create Virtual Environment**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

**Step 3: Install Dependencies**
```bash
pip install -r requirements.txt
```

**Step 4: Run**
```bash
python3 src/main.py
```

**Step 5: Deactivate When Done**
```bash
deactivate
```

**Advantage**: Lightweight, built into Python

---

### Option D: Direct Run (If Python & Dependencies Already Installed)

**Step 1: Clone Repository**
```bash
git clone https://github.com/Gayathri-Dinesh-27122003/F21BC_Coursework.git
cd F21BC_Coursework
```

**Step 2: Install Dependencies** (if needed)
```bash
pip install -r requirements.txt
```

**Step 3: Run**
```bash
python3 src/main.py
```

---

## ðŸ“Š Demo Workflow

When you run `python3 src/main.py`, here's what happens:

```
Step 1: Load Data
â”œâ”€ Load concrete dataset (824 train, 206 test)
â””â”€ Display: âœ“ Training set: 824 samples, 8 features

Step 2: Create Network
â”œâ”€ Architecture: [8 â†’ 64 â†’ 32 â†’ 1]
â”œâ”€ Activation: relu, relu, linear
â””â”€ Display: âœ“ Total parameters: 2689

Step 3: Calculate Initial RMSE
â”œâ”€ Random weights forward pass
â””â”€ Display: âœ“ Initial RMSE: 38.88

Step 4: Setup PSO
â”œâ”€ 30 particles
â”œâ”€ 3 informants per particle
â””â”€ Display: âœ“ Fitness function created

Step 5: Run Optimization
â”œâ”€ 50 iterations
â”œâ”€ Particles update positions
â””â”€ Display: Iteration 0-50 with best fitness values

Step 6: Evaluate Results
â”œâ”€ Apply optimized weights
â”œâ”€ Calculate train RMSE: 33.07
â”œâ”€ Calculate test RMSE: 31.80
â””â”€ Display:
    Final RMSE: 33.07
    Improvement: 14.93%
    âœ“ Good generalization
```

**Total Runtime**: 2-3 minutes

---

## ðŸ”¬ What This Demonstrates

### Algorithm Understanding
- âœ… PSO (Particle Swarm Optimization) implementation
- âœ… Neural network forward pass
- âœ… Fitness function creation for optimization
- âœ… Weight vector manipulation

### Software Engineering
- âœ… Clean code architecture (separated concerns)
- âœ… Reusable utility functions
- âœ… No code duplication
- âœ… Professional documentation

### Results
- âœ… 14.93% RMSE improvement from random initialization
- âœ… Good generalization (train-test gap < 2)
- âœ… Effective convergence within 50 iterations

---

## ðŸ“ Reproducibility

All results are reproducible:
- Fixed random seeds in experiments
- Documented dataset source and preprocessing
- Configuration parameters logged
- Results exported to CSV and JSON

**Verify Results**:
```bash
python3 src/main.py  # Run multiple times, get same results
```

---

## ðŸ”§ Customization

### Try Different Network Sizes
Edit `src/main.py` line 68:
```python
# Change from:
network = NeuralNetwork([n_features, 64, 32, 1], ['relu', 'relu', 'linear'])

# To:
network = NeuralNetwork([n_features, 128, 64, 32, 1], ['relu', 'relu', 'relu', 'linear'])
```

### Try Different PSO Parameters
Edit `src/main.py` line 83-88:
```python
pso = PSO(
    swarm_size=50,        # Change from 30
    max_iterations=100,   # Change from 50
    bounds=(-10.0, 10.0)  # Change bounds
)
```

---

## ðŸ“Š Expected Output

```
================================================================================
PSO-ANN INTEGRATION DEMO
Optimizing Neural Network Weights using Particle Swarm Optimization
================================================================================

ðŸ“Š Step 1: Loading Concrete Compressive Strength Dataset...
   âœ“ Training set:   824 samples, 8 features
   âœ“ Test set:       206 samples, 8 features

ðŸ§  Step 2: Creating Neural Network Architecture...
   âœ“ Architecture:         [8, 64, 32, 1]
   âœ“ Total parameters:     2689
   âœ“ Activation functions: ['relu', 'relu', 'linear']

ðŸ“ˆ Step 3: Calculating Initial Performance (Random Weights)...
   âœ“ Initial RMSE (training): 38.880100

âš™ï¸  Step 4: Setting up PSO Optimization...
   âœ“ Fitness function created (minimizes RMSE)

ðŸš€ Step 5: Running Particle Swarm Optimization...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸš€ Starting PSO Optimization
   Swarm size: 30
   Informants per particle: 3
   Search dimension: 2689
   Max iterations: 50
==================================================
   Iteration   0: Best Fitness = 765.647841
   Iteration  10: Best Fitness = 96.435521
   Iteration  20: Best Fitness = 52.097584
   Iteration  30: Best Fitness = 39.515320
   Iteration  40: Best Fitness = 35.125244
   Iteration  49: Best Fitness = 33.074428
==================================================
âœ… Optimization completed!
   Final best fitness: 33.074428
   Total iterations: 50

================================================================================
FINAL RESULTS
================================================================================

ðŸ“Š Performance Metrics:
   Initial RMSE (random weights):   38.880100
   Final RMSE (training set):       33.074428
   Final RMSE (test set):           31.802916

ðŸŽ¯ Optimization Results:
   RMSE Improvement:                14.93%
   PSO Iterations:                  50
   Best Fitness Value:              33.074428

ðŸ“ˆ Generalization:
   Train-Test Gap:                  -1.271512
   Status:                          âœ“ Good generalization

================================================================================

ðŸ’¾ Results Summary saved

âœ¨ Demo completed successfully!
```

---

## â“ FAQ

**Q: Do I need to install Python?**
A: Only with Option D. Docker (Option A) includes Python automatically.

**Q: Can I run this on Windows/Mac/Linux?**
A: Yes, all options work on all platforms.

**Q: How long does it take?**
A: 2-3 minutes for the demo, 15-20 minutes for 10 experiments.

**Q: What's the advantage of each option?**
- Docker: No dependencies, guaranteed reproducibility
- Conda: Easy environment management
- Virtual Environment: Lightweight, standard Python
- Direct: Fastest if dependencies already installed

**Q: Can I modify the code?**
A: Yes, edit `src/main.py` to customize network size, PSO parameters, etc.

**Q: Where's the data from?**
A: UCI ML Repository - Concrete Compressive Strength dataset (1030 samples)

**Q: Why PSO instead of backpropagation?**
A: This is a demonstration of metaheuristic optimization vs gradient descent.

---

## ðŸ“ž Summary

| Aspect | Details |
|--------|---------|
| **Main Demo** | `python3 src/main.py` (2-3 min) |
| **Batch Experiments** | `experiment_runner.ipynb` (15-20 min) |
| **Utilities** | `pso_ann_trainer.py` (62 lines) |
| **Network** | `ann.py` (241 lines) |
| **Optimizer** | `pso.py` (175 lines) |
| **Best Setup** | Docker (no dependencies) |
| **Results** | ~15% RMSE improvement |
| **Reproducible** | âœ… Yes (fixed seeds) |
| **Status** | âœ… Ready for submission |

---

**Choose an option above and run the code. It's ready to go!**
